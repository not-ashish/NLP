{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ashish_ashok_IHLT_STS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5KJb2u89tlg",
        "outputId": "a62c5bd8-e5e1-4b6a-bf55-d29c7afff780"
      },
      "source": [
        "import nltk\n",
        "import random, math, copy, os, string\n",
        "from scipy.stats import pearsonr\n",
        "from nltk.metrics import jaccard_distance\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn  \n",
        "from nltk.corpus import wordnet_ic\n",
        "from nltk.wsd import lesk\n",
        "import nltk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('conll2000')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('words')\n",
        "!pip install svgling\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('universal_tagset') \n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('wordnet_ic')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "Collecting svgling\n",
            "  Downloading svgling-0.3.1-py3-none-any.whl (21 kB)\n",
            "Collecting svgwrite\n",
            "  Downloading svgwrite-1.4.1-py3-none-any.whl (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 4.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
            "Successfully installed svgling-0.3.1 svgwrite-1.4.1\n",
            "Mounted at /content/drive\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW27RTBb9yTP",
        "outputId": "ec7ab3c1-a97a-4f2a-f9b6-96cf510c1361"
      },
      "source": [
        "import pandas as pd\n",
        "dt = pd.read_csv('/content/drive/My Drive/content/drive/test-gold/STS.input.SMTeuroparl.txt',sep='\\t',header=None)\n",
        "dt['gs'] = pd.read_csv('/content/drive/My Drive/content/drive/test-gold/STS.gs.SMTeuroparl.txt',sep='\\t',header=None)\n",
        "dt.head()\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "GV3P2nn7blID",
        "outputId": "ba959970-8167-452e-c56f-c3a3ac125c56"
      },
      "source": [
        "#Document structure\n",
        "sw = nltk.corpus.stopwords.words('english')\n",
        "Jaccard_Dist=[] #Here we will store the Jaccard Distances between the pairs\n",
        "tok=[] #Here we will keep a tokenized version of the pairs of phrases\n",
        "for i in range(len(dt)):\n",
        "  sent=[set(nltk.word_tokenize(dt[0][i])),set(nltk.word_tokenize(dt[1][i]))] \n",
        "  for n in [0,1]:\n",
        "    sent[n]=set([w.lower() for w in sent[n] if w.lower() not in sw and w not in string.punctuation and w.isalpha()]) \n",
        "  tok.append(sent)\n",
        "  Jaccard_Dist.append(jaccard_distance(sent[0],sent[1]))\n",
        "  \n",
        "dt['Jaccard']=Jaccard_Dist \n",
        "dt.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>gs</th>\n",
              "      <th>Jaccard</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The leaders have now been given a new chance a...</td>\n",
              "      <td>The leaders benefit aujourd' hui of a new luck...</td>\n",
              "      <td>4.50</td>\n",
              "      <td>0.692308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Amendment No 7 proposes certain changes in the...</td>\n",
              "      <td>Amendment No 7 is proposing certain changes in...</td>\n",
              "      <td>5.00</td>\n",
              "      <td>0.285714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Let me remind you that our allies include ferv...</td>\n",
              "      <td>I would like to remind you that among our alli...</td>\n",
              "      <td>4.25</td>\n",
              "      <td>0.727273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The vote will take place today at 5.30 p.m.</td>\n",
              "      <td>The vote will take place at 5.30pm</td>\n",
              "      <td>4.50</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The fishermen are inactive, tired and disappoi...</td>\n",
              "      <td>The fishermen are inactive, tired and disappoi...</td>\n",
              "      <td>5.00</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0  ...   Jaccard\n",
              "0  The leaders have now been given a new chance a...  ...  0.692308\n",
              "1  Amendment No 7 proposes certain changes in the...  ...  0.285714\n",
              "2  Let me remind you that our allies include ferv...  ...  0.727273\n",
              "3        The vote will take place today at 5.30 p.m.  ...  0.250000\n",
              "4  The fishermen are inactive, tired and disappoi...  ...  0.000000\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWVFv2Akc4rI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "507223f3-173d-4e45-dd07-0a706e24ed3d"
      },
      "source": [
        "tok=[] #Here we will keep a tokenized version of the pairs of phrases\n",
        "for i in range(len(dt)):\n",
        "  sent=[set(nltk.word_tokenize(dt[0][i])),set(nltk.word_tokenize(dt[1][i]))] \n",
        "  tok.append(sent)\n",
        "\n",
        "PoS=[] #Here we will store the PoS tags\n",
        "\n",
        "for i in range(len(dt)):\n",
        "   PoS.append([set(nltk.pos_tag(tok[i][0])),set(nltk.pos_tag(tok[i][1]))])\n",
        "\n",
        "lem=[]\n",
        "lem.append(nltk.ne_chunk_sents(PoS))\n",
        "\n",
        "print(lem)  \n",
        "\"\"\"  \n",
        "Lem_Dist=[]\n",
        "for i in range(len(dt)):\n",
        "  Lem_Dist.append(jaccard_distance(Lemmas[i][0],Lemmas[i][1]))\n",
        "dt['Lemma']=Lem_Dist\n",
        "dt.head()\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<generator object ParserI.parse_sents.<locals>.<genexpr> at 0x7fbe5a3884d0>]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"  \\nLem_Dist=[]\\nfor i in range(len(dt)):\\n  Lem_Dist.append(jaccard_distance(Lemmas[i][0],Lemmas[i][1]))\\ndt['Lemma']=Lem_Dist\\ndt.head()\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE2TsGYdby8q",
        "outputId": "8f6a7811-e2f4-433d-8d6e-2b0e558b6bb8"
      },
      "source": [
        "sentence = \"Mark Pedersen and John Smith are working at Google \" + \\\n",
        "           \"since 1994 for $1000 per week.\"\n",
        "\n",
        "x = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
        "\n",
        "res = nltk.ne_chunk(x)\n",
        "print(res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Mark/NNP)\n",
            "  (ORGANIZATION Pedersen/NNP)\n",
            "  and/CC\n",
            "  (PERSON John/NNP Smith/NNP)\n",
            "  are/VBP\n",
            "  working/VBG\n",
            "  at/IN\n",
            "  (ORGANIZATION Google/NNP)\n",
            "  since/IN\n",
            "  1994/CD\n",
            "  for/IN\n",
            "  $/$\n",
            "  1000/CD\n",
            "  per/IN\n",
            "  week/NN\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "na6AoaeMdLl0",
        "outputId": "bade7023-c742-494d-a761-074196f2b681"
      },
      "source": [
        "\n",
        "# Create the list to store Part of Speech Tags by nltk standard that wordnet deals with\n",
        "Admissible_PoS=['NOUN', 'VERB', 'ADJ', 'ADV']\n",
        "\n",
        "#Now we are going to change the Part of Speech tag to the format that Wordnet uses\n",
        "pos_in_wn = {'DET':None, 'NOUN':'n', 'VERB':'v', 'ADJ':'a', 'ADV': 'r' , 'ADP':None, 'CONJ':None, 'NUM':None, 'PRT':None, 'PRON':None, '.':None, 'X':None }\n",
        "\n",
        "#Tokenize the sentences\n",
        "tokenized_sents = []\n",
        "for i in range(len(dt)):\n",
        "  sent = [nltk.word_tokenize(dt[0][i]),nltk.word_tokenize(dt[1][i])] \n",
        "  tokenized_sents.append(sent)\n",
        "tokenized_sents\n",
        "\n",
        "#First we apply the lesk algorithm, and keep the synsets\n",
        "lesk = copy.deepcopy(tok)\n",
        "for i, sent in enumerate(lesk): \n",
        " for n in range(2):\n",
        "   sent[n]=nltk.pos_tag(sent[n],tagset='universal')   \n",
        "   sent[n] = set([nltk.ne_chunk(tokenized_sents[i][n], w[0],pos_in_wn[w[1]]) for w in sent[n] if pos_in_wn[w[1]] is not None])\n",
        "\n",
        "\n",
        "#Calculating the Jaccard distance for the resulting synsets \n",
        "Jaccard_Lesk = []\n",
        "for sent in lesk:\n",
        "  Jaccard_Lesk.append(jaccard_distance(sent[0],sent[1]))\n",
        "dt['Lesk']=Jaccard_Lesk \n",
        "\n",
        "dt.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-46efeee43d22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m    \u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtagset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'universal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m    \u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos_in_wn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpos_in_wn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-46efeee43d22>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m    \u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtagset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'universal'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m    \u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_sents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos_in_wn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpos_in_wn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ne_chunk() takes from 1 to 2 positional arguments but 3 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0b764t_eBJO",
        "outputId": "6bed3922-ca3c-4f90-fd57-7f6a0ce79d76"
      },
      "source": [
        "print([pearsonr(dt['gs'], dt['Jaccard'])[0],pearsonr(dt['gs'], dt['Lemma'])[0], pearsonr(dt['gs'], dt['Lesk'])[0]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.45279693414784167, -0.47130316108106723, -0.3540038658213318]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVsf2b6ZegkO"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkishV1jfL4M"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvLru6BSfQKn",
        "outputId": "ed16b44d-54a3-4286-d4f5-932c149ea8b6"
      },
      "source": [
        "print([pearsonr(dt['Lesk'], dt['Jaccard'])[0]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.8349367273950777]\n"
          ]
        }
      ]
    }
  ]
}