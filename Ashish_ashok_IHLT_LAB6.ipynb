{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ashish_ashok_IHLT_LAB6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5KJb2u89tlg",
        "outputId": "13a1e127-1172-47ee-e228-7f5f299212d7"
      },
      "source": [
        "import nltk\n",
        "import random, math, copy, os, string\n",
        "from scipy.stats import pearsonr\n",
        "from nltk.metrics import jaccard_distance\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn  \n",
        "from nltk.corpus import wordnet_ic\n",
        "from nltk.wsd import lesk\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('universal_tagset') \n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('wordnet_ic')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW27RTBb9yTP",
        "outputId": "a38be3b1-0f6d-401f-db96-ca6b5096da82"
      },
      "source": [
        "import pandas as pd\n",
        "dt = pd.read_csv('/content/drive/My Drive/content/drive/test-gold/STS.input.SMTeuroparl.txt',sep='\\t',header=None)\n",
        "dt['gs'] = pd.read_csv('/content/drive/My Drive/content/drive/test-gold/STS.gs.SMTeuroparl.txt',sep='\\t',header=None)\n",
        "dt.head()\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV3P2nn7blID"
      },
      "source": [
        "#Document structure\n",
        "sw = nltk.corpus.stopwords.words('english')\n",
        "Jaccard_Dist=[] #Here we will store the Jaccard Distances between the pairs\n",
        "tok=[] #Here we will keep a tokenized version of the pairs of phrases\n",
        "for i in range(len(dt)):\n",
        "  sent=[set(nltk.word_tokenize(dt[0][i])),set(nltk.word_tokenize(dt[1][i]))] \n",
        "  for n in [0,1]:\n",
        "    sent[n]=set([w.lower() for w in sent[n] if w.lower() not in sw and w not in string.punctuation and w.isalpha()]) \n",
        "  tok.append(sent)\n",
        "  Jaccard_Dist.append(jaccard_distance(sent[0],sent[1]))\n",
        "  \n",
        "dt['Jaccard']=Jaccard_Dist \n",
        "dt.head()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWVFv2Akc4rI"
      },
      "source": [
        "#morphology\n",
        "def lemmatize(p):\n",
        "    if p[1][0] in {'N','V'}:\n",
        "        return wnl.lemmatize(p[0], pos=p[1][0].lower()) \n",
        "    return p[0]\n",
        "\n",
        "wnl = nltk.stem.WordNetLemmatizer() #We will use this lemmatizer\n",
        "PoS=[] #Here we will store the PoS tags\n",
        "\n",
        "for i in range(len(dt)):\n",
        "   PoS.append([set(nltk.pos_tag(tok[i][0])),set(nltk.pos_tag(tok[i][1]))])\n",
        "\n",
        "Lemmas=[] \n",
        "lem=[] #auxiliary vector\n",
        "for i in range(len(dt)): \n",
        "  for n in [0,1]:\n",
        "    lem.append(set([lemmatize(w) for w in PoS[i][n]]))\n",
        "  Lemmas.append(lem.copy())\n",
        "  lem.clear()\n",
        "del lem\n",
        "\n",
        "Lem_Dist=[]\n",
        "for i in range(len(dt)):\n",
        "  Lem_Dist.append(jaccard_distance(Lemmas[i][0],Lemmas[i][1]))\n",
        "dt['Lemma']=Lem_Dist\n",
        "dt.head()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "na6AoaeMdLl0",
        "outputId": "9fe39cc5-c00f-47ee-80b4-d4191953e6a2"
      },
      "source": [
        "#Lesk algorithm\n",
        "\n",
        "# Create the list to store Part of Speech Tags by nltk standard that wordnet deals with\n",
        "Admissible_PoS=['NOUN', 'VERB', 'ADJ', 'ADV']\n",
        "\n",
        "#Now we are going to change the Part of Speech tag to the format that Wordnet uses\n",
        "pos_in_wn = {'DET':None, 'NOUN':'n', 'VERB':'v', 'ADJ':'a', 'ADV': 'r' , 'ADP':None, 'CONJ':None, 'NUM':None, 'PRT':None, 'PRON':None, '.':None, 'X':None }\n",
        "\n",
        "#Tokenize the sentences\n",
        "tokenized_sents = []\n",
        "for i in range(len(dt)):\n",
        "  sent = [nltk.word_tokenize(dt[0][i]),nltk.word_tokenize(dt[1][i])] \n",
        "  tokenized_sents.append(sent)\n",
        "\n",
        "#First we apply the lesk algorithm, and keep the synsets\n",
        "lesk = copy.deepcopy(tok)\n",
        "for i, sent in enumerate(lesk): \n",
        " for n in range(2):\n",
        "   sent[n]=nltk.pos_tag(sent[n],tagset='universal')   \n",
        "   sent[n] = set([nltk.wsd.lesk(tokenized_sents[i][n], w[0],pos_in_wn[w[1]]) for w in sent[n] if pos_in_wn[w[1]] is not None])\n",
        "\n",
        "\n",
        "#Calculating the Jaccard distance for the resulting synsets \n",
        "Jaccard_Lesk = []\n",
        "for sent in lesk:\n",
        "  Jaccard_Lesk.append(jaccard_distance(sent[0],sent[1]))\n",
        "dt['Lesk']=Jaccard_Lesk \n",
        "\n",
        "dt.head()\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>gs</th>\n",
              "      <th>Jaccard</th>\n",
              "      <th>Lemma</th>\n",
              "      <th>Lesk</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The leaders have now been given a new chance a...</td>\n",
              "      <td>The leaders benefit aujourd' hui of a new luck...</td>\n",
              "      <td>4.50</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.555556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Amendment No 7 proposes certain changes in the...</td>\n",
              "      <td>Amendment No 7 is proposing certain changes in...</td>\n",
              "      <td>5.00</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Let me remind you that our allies include ferv...</td>\n",
              "      <td>I would like to remind you that among our alli...</td>\n",
              "      <td>4.25</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.625000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The vote will take place today at 5.30 p.m.</td>\n",
              "      <td>The vote will take place at 5.30pm</td>\n",
              "      <td>4.50</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The fishermen are inactive, tired and disappoi...</td>\n",
              "      <td>The fishermen are inactive, tired and disappoi...</td>\n",
              "      <td>5.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0  ...      Lesk\n",
              "0  The leaders have now been given a new chance a...  ...  0.555556\n",
              "1  Amendment No 7 proposes certain changes in the...  ...  0.500000\n",
              "2  Let me remind you that our allies include ferv...  ...  0.625000\n",
              "3        The vote will take place today at 5.30 p.m.  ...  0.250000\n",
              "4  The fishermen are inactive, tired and disappoi...  ...  0.000000\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0b764t_eBJO",
        "outputId": "6bed3922-ca3c-4f90-fd57-7f6a0ce79d76"
      },
      "source": [
        "print([pearsonr(dt['gs'], dt['Jaccard'])[0],pearsonr(dt['gs'], dt['Lemma'])[0], pearsonr(dt['gs'], dt['Lesk'])[0]])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.45279693414784167, -0.47130316108106723, -0.3540038658213318]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVsf2b6ZegkO"
      },
      "source": [
        "4)From the above pearson correlation we can conclude that lesk's algorithm in comparison to document structure and morphology is very inefficient and the results are mostly empty."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkishV1jfL4M"
      },
      "source": [
        "5)Compare the results with gold standard by giving the pearson correlation between them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvLru6BSfQKn",
        "outputId": "ed16b44d-54a3-4286-d4f5-932c149ea8b6"
      },
      "source": [
        "print([pearsonr(dt['Lesk'], dt['Jaccard'])[0]])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8349367273950777]\n"
          ]
        }
      ]
    }
  ]
}